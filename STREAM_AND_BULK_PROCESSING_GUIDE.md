# Stream and Bulk Processing: Complete Implementation Guide

## ðŸ“š Table of Contents
1. [Overview](#overview)
2. [Stream Processing](#stream-processing)
3. [Bulk Processing](#bulk-processing)
4. [When to Use What](#when-to-use-what)
5. [Technologies and Tools](#technologies-and-tools)
6. [Implementation in Codebase](#implementation-in-codebase)
7. [Best Practices](#best-practices)
8. [Strategies and Patterns](#strategies-and-patterns)
9. [Downstream Data Usage](#downstream-data-usage)
10. [Real-World Examples](#real-world-examples)
11. [Performance Tuning](#performance-tuning)
12. [Monitoring and Observability](#monitoring-and-observability)
13. [Error Handling Strategies](#error-handling-strategies)
14. [Data Consistency Patterns](#data-consistency-patterns)
15. [Scaling Strategies](#scaling-strategies)
16. [Cost Optimization](#cost-optimization)
17. [Troubleshooting](#troubleshooting)
18. [Interview Questions](#interview-questions)

---

## Overview

### What is Stream Processing?
Stream processing is the real-time processing of continuous data streams. Data is processed as it arrives, enabling immediate insights and actions.

**Key Characteristics:**
- **Real-time**: Processes data as it arrives (milliseconds to seconds latency)
- **Continuous**: Data flows continuously, not in batches
- **Event-driven**: Reacts to events as they happen
- **Low latency**: Sub-second to second-level processing

### What is Bulk Processing?
Bulk processing (batch processing) handles large volumes of data in discrete batches at scheduled intervals or on-demand.

**Key Characteristics:**
- **Scheduled**: Runs at specific intervals (hourly, daily, etc.)
- **High throughput**: Processes large volumes efficiently
- **Resource-intensive**: Can use significant compute resources
- **Higher latency**: Minutes to hours for completion

### Key Differences

| Aspect | Stream Processing | Bulk Processing |
|--------|------------------|-----------------|
| **Latency** | Milliseconds to seconds | Minutes to hours |
| **Data Flow** | Continuous | Discrete batches |
| **Use Cases** | Real-time monitoring, alerts, fraud detection | ETL, reporting, analytics |
| **Complexity** | Higher (state management, ordering) | Lower (simpler logic) |
| **Resource Usage** | Constant (always running) | Periodic (spikes) |

---

## Stream Processing

### Concepts

#### 1. Event Streams
A continuous flow of events (messages) that represent business occurrences.

**Example:**
```
OrderCreated â†’ PaymentProcessed â†’ InventoryReserved â†’ OrderShipped
```

#### 2. Stream Processing Patterns

**a) Filtering**
- Remove unwanted events
- Example: Filter orders above $1000

**b) Transformation**
- Convert event format
- Example: Convert order event to analytics format

**c) Aggregation**
- Calculate metrics over time windows
- Example: Count orders per minute, calculate average order value

**d) Joining**
- Combine multiple streams
- Example: Join order events with customer data

**e) Windowing**
- Group events by time or count
- Example: 5-minute windows, 1000-event windows

#### 3. State Management
Stream processing often requires maintaining state:
- **Keyed State**: State per key (e.g., per customer)
- **Operator State**: State for operators (e.g., counters)
- **Window State**: State for time windows

---

## Bulk Processing

### Concepts

#### 1. Batch Jobs
Discrete processing jobs that run on scheduled intervals or triggers.

**Types:**
- **Scheduled**: Cron-based (daily, hourly)
- **Triggered**: Event-driven (file arrival, API call)
- **On-demand**: Manual execution

#### 2. ETL (Extract, Transform, Load)
- **Extract**: Read data from sources
- **Transform**: Clean, enrich, aggregate
- **Load**: Write to destination

#### 3. Batch Processing Patterns

**a) Full Load**
- Process entire dataset
- Example: Daily full data refresh

**b) Incremental Load**
- Process only new/changed data
- Example: Process only orders from last hour

**c) Delta Processing**
- Process changes (inserts, updates, deletes)
- Example: Process only modified products

**d) Partitioning**
- Process data in partitions
- Example: Process by date, region, or customer segment

---

## When to Use What

### Use Stream Processing When:
1. **Real-time requirements**: Need immediate processing (< 1 second)
2. **Continuous data**: Data arrives continuously
3. **Event-driven actions**: Need to react immediately
4. **Low latency critical**: Fraud detection, monitoring, alerts
5. **High-frequency events**: Thousands to millions per second

**Examples:**
- Real-time fraud detection
- Live dashboards
- Real-time recommendations
- Order processing pipeline
- IoT sensor data processing
- Clickstream analytics

### Use Bulk Processing When:
1. **Scheduled requirements**: Daily/hourly reports
2. **Large volumes**: Process terabytes efficiently
3. **Complex transformations**: Heavy computations
4. **Cost optimization**: Can use cheaper compute resources
5. **Historical analysis**: Process historical data

**Examples:**
- Daily sales reports
- Monthly financial reconciliation
- Data warehouse ETL
- Customer segmentation
- Historical data analysis
- Data migration

### Hybrid Approach (Lambda Architecture)
Use both:
- **Stream**: Real-time processing for immediate insights
- **Batch**: Corrective processing for accuracy

**Example:**
- Stream: Real-time order count (may have duplicates)
- Batch: Daily accurate order count (deduplicated)

---

## Technologies and Tools

### Stream Processing Technologies

#### 1. Apache Kafka
**What it is:** Distributed event streaming platform

**Key Features:**
- High throughput (millions of messages/second)
- Fault tolerance (replication)
- Scalability (partitions)
- Durability (persistent storage)

**Use Cases:**
- Event-driven microservices
- Real-time data pipelines
- Log aggregation
- Activity tracking

**Example:**
```java
// Producer
kafkaTemplate.send("order-created", orderId, event);

// Consumer
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void processOrder(OrderCreatedEvent event) {
    // Process order
}
```

#### 2. Kafka Streams
**What it is:** Stream processing library for Kafka

**Key Features:**
- Built on Kafka
- Stateful processing
- Windowing and aggregations
- Exactly-once semantics

**Use Cases:**
- Real-time aggregations
- Stream transformations
- Event enrichment
- Real-time analytics

**Example:**
```java
KStream<String, OrderEvent> orders = builder.stream("orders");
orders
    .filter((key, order) -> order.getAmount() > 1000)
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
    .count()
    .toStream()
    .to("high-value-orders");
```

#### 3. Apache Flink
**What it is:** Distributed stream processing framework

**Key Features:**
- Low latency (milliseconds)
- High throughput
- Stateful processing
- Event time processing
- Exactly-once guarantees

**Use Cases:**
- Complex event processing
- Real-time analytics
- Fraud detection
- IoT data processing

**Example:**
```java
DataStream<OrderEvent> orders = env.addSource(kafkaSource);
orders
    .keyBy(OrderEvent::getCustomerId)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
    .aggregate(new OrderAggregator())
    .addSink(new AnalyticsSink());
```

#### 4. Apache Pulsar
**What it is:** Cloud-native messaging and streaming

**Key Features:**
- Multi-tenancy
- Geo-replication
- Unified messaging model
- Built-in functions

**Use Cases:**
- Multi-tenant systems
- Global data distribution
- Event streaming

---

### Bulk Processing Technologies

#### 1. Apache Spark
**What it is:** Unified analytics engine for large-scale data processing

**Key Features:**
- Batch and stream processing
- In-memory computing
- Fault tolerance
- Supports SQL, ML, Graph processing

**Batch Processing Example:**
```scala
val orders = spark.read.parquet("s3://bucket/orders/")
val dailySales = orders
    .filter($"date" === currentDate())
    .groupBy("product_id")
    .agg(sum("amount").alias("total_sales"))
    .write.mode("overwrite")
    .parquet("s3://bucket/daily-sales/")
```

**Stream Processing Example (Spark Streaming):**
```scala
val stream = spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "orders")
    .load()

val processed = stream
    .selectExpr("CAST(value AS STRING)")
    .as[OrderEvent]
    .groupBy(window($"timestamp", "5 minutes"), $"product_id")
    .agg(sum("amount").alias("total"))

processed.writeStream
    .format("parquet")
    .option("path", "s3://bucket/streaming-output/")
    .start()
```

#### 2. Apache Airflow
**What it is:** Workflow orchestration platform

**Key Features:**
- DAG-based workflows
- Scheduling
- Monitoring
- Extensible

**Use Cases:**
- ETL pipelines
- Data pipeline orchestration
- Scheduled jobs

**Example:**
```python
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG('daily_etl', schedule_interval='@daily')

extract = BashOperator(
    task_id='extract',
    bash_command='python extract_orders.py',
    dag=dag
)

transform = BashOperator(
    task_id='transform',
    bash_command='spark-submit transform.py',
    dag=dag
)

load = BashOperator(
    task_id='load',
    bash_command='python load_to_warehouse.py',
    dag=dag
)

extract >> transform >> load
```

#### 3. Snowflake
**What it is:** Cloud data warehouse

**Key Features:**
- Separation of storage and compute
- Automatic scaling
- Time travel
- Zero-copy cloning

**Use Cases:**
- Data warehousing
- Analytics
- Data sharing
- ELT pipelines

**Example:**
```sql
-- Bulk load from S3
COPY INTO orders
FROM 's3://bucket/orders/'
FILE_FORMAT = (TYPE = 'PARQUET');

-- Transform and aggregate
CREATE TABLE daily_sales AS
SELECT 
    DATE(order_date) as sale_date,
    product_id,
    SUM(amount) as total_sales,
    COUNT(*) as order_count
FROM orders
WHERE order_date >= CURRENT_DATE() - 7
GROUP BY DATE(order_date), product_id;

-- Stream processing with Snowpipe
CREATE PIPE order_pipe
AUTO_INGEST = TRUE
AS
COPY INTO orders
FROM @s3_stage
FILE_FORMAT = (TYPE = 'JSON');
```

#### 4. Apache Beam
**What it is:** Unified programming model for batch and stream processing

**Key Features:**
- Write once, run anywhere
- Supports multiple runners (Spark, Flink, Dataflow)
- Unified API

**Example:**
```java
Pipeline pipeline = Pipeline.create();
PCollection<OrderEvent> orders = pipeline
    .apply(Read.from(new KafkaIO.Read<String, OrderEvent>()
        .withBootstrapServers("localhost:9092")
        .withTopic("orders")));

orders
    .apply(Filter.by(order -> order.getAmount() > 1000))
    .apply(Window.into(FixedWindows.of(Duration.standardMinutes(5))))
    .apply(Sum.integersPerKey())
    .apply(Write.to(new TextIO.Write().to("output")));

pipeline.run();
```

---

## Implementation in Codebase

### Current Stream Processing Implementation

#### 1. Kafka Configuration
**Location:** `product-service/src/main/java/com/ekart/product_service/config/KafkaConfig.java`

**Key Configuration:**
```java
@Configuration
public class KafkaConfig {
    // Topic names
    public static final String ORDER_CREATED_TOPIC = "order-created";
    public static final String INVENTORY_RESERVED_TOPIC = "inventory-reserved";
    public static final String PAYMENT_PROCESSED_TOPIC = "payment-processed";
    public static final String ORDER_COMPLETED_TOPIC = "order-completed";
    public static final String ORDER_CANCELLED_TOPIC = "order-cancelled";

    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        configProps.put(ProducerConfig.ACKS_CONFIG, "all"); // Wait for all replicas
        configProps.put(ProducerConfig.RETRIES_CONFIG, 3);
        configProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true); // Prevent duplicates
        configProps.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public NewTopic orderCreatedTopic() {
        return TopicBuilder.name(ORDER_CREATED_TOPIC)
                .partitions(3)
                .replicas(1)
                .build();
    }
}
```

**Configuration in application.yaml:**
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all
      retries: 3
      enable-idempotence: true
      max-in-flight-requests-per-connection: 1
    consumer:
      group-id: product-service-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      auto-offset-reset: earliest
      properties:
        spring.json.trusted.packages: "*"
```

**Key Settings Explained:**
- **acks: all**: Wait for all replicas to acknowledge (highest durability)
- **enable-idempotence: true**: Prevents duplicate messages (exactly-once semantics)
- **max-in-flight-requests-per-connection: 1**: Required for idempotence
- **auto-offset-reset: earliest**: Start from beginning if no offset exists

#### 2. Kafka Event Publishing
**Location:** `product-service/src/main/java/com/ekart/product_service/event/OrderEventPublisher.java`

**What it does:**
- Publishes order events to Kafka topics
- Uses async publishing with CompletableFuture
- Handles errors and logging
- Provides idempotent event publishing

**Code:**
```java
@Service
public class OrderEventPublisher {
    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;

    public void publishOrderCreated(String orderId, Long buyerId, 
                                   List<OrderItem> items,
                                   BigDecimal totalAmount, 
                                   String shippingAddress) {
        OrderCreatedEvent event = new OrderCreatedEvent(
            orderId, buyerId, items, totalAmount, 
            LocalDateTime.now(), shippingAddress
        );

        try {
            CompletableFuture<SendResult<String, Object>> future = 
                kafkaTemplate.send(KafkaConfig.ORDER_CREATED_TOPIC, orderId, event);
            
            future.whenComplete((result, ex) -> {
                if (ex == null) {
                    logger.info("OrderCreated event published: orderId={}, offset={}", 
                        orderId, result.getRecordMetadata().offset());
                } else {
                    logger.error("Error publishing OrderCreated event: orderId={}", 
                        orderId, ex);
                }
            });
        } catch (Exception e) {
            logger.error("Error publishing OrderCreated event: orderId={}", orderId, e);
            throw new EventPublishException("Failed to publish OrderCreated event", e);
        }
    }

    // Similar methods for other events:
    // - publishInventoryReserved()
    // - publishPaymentProcessed()
    // - publishOrderCompleted()
    // - publishOrderCancelled()
}
```

**Topics Used:**
- `order-created`: When order is placed (3 partitions, 1 replica)
- `inventory-reserved`: When inventory is reserved (3 partitions, 1 replica)
- `payment-processed`: When payment is processed (3 partitions, 1 replica)
- `order-completed`: When order is completed (3 partitions, 1 replica)
- `order-cancelled`: When order is cancelled (3 partitions, 1 replica)

**Event Flow:**
```
Order Service â†’ publishOrderCreated() â†’ Kafka Topic (order-created)
    â†“
Multiple Consumers (different consumer groups):
    - inventory-group: Reserve inventory
    - payment-group: Process payment
    - analytics-group: Update metrics
    - notification-group: Send notifications
```

#### 3. Kafka Event Consumption
**Location:** Various services with `@KafkaListener`

**Basic Pattern:**
```java
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void handleOrderCreated(OrderCreatedEvent event) {
    logger.info("Received OrderCreated event: orderId={}", event.getOrderId());
    try {
        inventoryService.reserveInventory(event);
    } catch (Exception e) {
        logger.error("Error processing order: orderId={}", event.getOrderId(), e);
        // Handle error (retry, DLQ, etc.)
    }
}
```

**Advanced Pattern with Headers:**
```java
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void handleOrderCreated(
    @Payload OrderCreatedEvent event,
    @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
    @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
    @Header(KafkaHeaders.OFFSET) long offset,
    @Header(KafkaHeaders.RECEIVED_KEY) String key
) {
    logger.info("Received event from topic={}, partition={}, offset={}, key={}", 
        topic, partition, offset, key);
    
    // Process event
    inventoryService.reserveInventory(event);
}
```

**Manual Acknowledgment:**
```java
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void handleOrderCreated(
    OrderCreatedEvent event,
    Acknowledgment ack
) {
    try {
        inventoryService.reserveInventory(event);
        ack.acknowledge();  // Acknowledge after successful processing
    } catch (Exception e) {
        logger.error("Error processing order: orderId={}", event.getOrderId(), e);
        // Don't acknowledge - will be retried
    }
}
```

**Consumer Groups:**
- `inventory-group`: Inventory service consumers (reserve inventory on order)
- `payment-group`: Payment service consumers (process payment)
- `order-group`: Order service consumers (order state management)
- `notification-group`: Notification service consumers (send emails/SMS)
- `analytics-group`: Analytics service consumers (update metrics)

**Consumer Group Behavior:**
- Each consumer group processes all messages independently
- Within a group, each partition is consumed by only one consumer
- Adding consumers to a group increases parallelism (up to partition count)

### Current Bulk Processing Implementation

#### 1. Data Seeding (Batch Operations)
**Location:** `seller-service/src/main/java/com/ekart/seller_service/service/DataSeederService.java`

**What it does:**
- Generates bulk test data on application startup
- Uses batch processing for efficiency
- Processes sellers, products, orders in batches
- Configurable via application.yaml

**Configuration:**
```yaml
data:
  seeder:
    enabled: true
    sellers: 100
    spus-per-seller: 10
    skus-per-spu: 3
    orders-per-seller: 10
    warehouses: 5
```

**Implementation Pattern:**
```java
@Component
public class DataSeederService implements CommandLineRunner {
    
    @Override
    @Transactional
    public void run(String... args) {
        if (!dataSeederConfig.isEnabled()) {
            logger.info("Data seeder is disabled");
            return;
        }

        logger.info("Starting data seeding...");
        long startTime = System.currentTimeMillis();

        // Generate sellers
        List<Seller> sellers = generateSellers();
        logger.info("Generated {} sellers", sellers.size());

        // Process each seller's data in batches
        for (Seller seller : sellers) {
            // Generate SPUs (Standard Product Units)
            List<SPU> spus = generateSPUs(seller);
            
            // Generate SKUs (Stock Keeping Units) for each SPU
            List<SKU> skus = new ArrayList<>();
            for (SPU spu : spus) {
                List<SKU> spuSkus = generateSKUs(seller, spu);
                skus.addAll(spuSkus);
                
                // Generate inventory for each SKU
                for (SKU sku : spuSkus) {
                    generateInventory(seller, sku);
                }
            }
            
            // Generate orders
            List<Order> orders = generateOrders(seller, skus);
            
            // Generate payouts for completed orders
            List<Payout> payouts = generatePayouts(seller, orders);
        }

        long duration = System.currentTimeMillis() - startTime;
        logger.info("Data seeding completed in {} ms", duration);
    }
}
```

**Batch Processing Benefits:**
- **Efficiency**: Processes multiple records in single transaction
- **Performance**: Reduces database round trips
- **Consistency**: All-or-nothing transaction semantics
- **Scalability**: Can process large datasets

**Performance Metrics:**
- 100 sellers with default config: ~10-30 seconds
- Generates ~11,400+ records total:
  - 100 sellers
  - 1,000 SPUs (100 Ã— 10)
  - 3,000 SKUs (1,000 Ã— 3)
  - ~6,000 inventory records
  - 1,000 orders
  - ~400 payouts

#### 2. Bulk Database Operations Pattern

**JPA Batch Insert:**
```java
@Transactional
public void bulkInsertProducts(List<Product> products) {
    int batchSize = 1000;
    for (int i = 0; i < products.size(); i += batchSize) {
        List<Product> batch = products.subList(
            i, Math.min(i + batchSize, products.size())
        );
        productRepository.saveAll(batch);
        productRepository.flush();  // Flush to database
        entityManager.clear();      // Clear persistence context
    }
}
```

**Configuration for Batch Operations:**
```yaml
spring:
  jpa:
    properties:
      hibernate:
        jdbc:
          batch_size: 1000
        order_inserts: true
        order_updates: true
        jdbc.batch_versioned_data: true
```

**Benefits:**
- Reduces database round trips
- Improves performance for large inserts
- Maintains transaction consistency

---

## Best Practices

### Stream Processing Best Practices

#### 1. Event Design
- **Idempotency**: Make operations idempotent
- **Schema Evolution**: Use schema registry (Avro, Protobuf)
- **Event Sourcing**: Store events as source of truth

**Example:**
```java
// Idempotent event processing
@KafkaListener(topics = "order-created")
public void processOrder(OrderCreatedEvent event) {
    // Check if already processed
    if (orderRepository.existsById(event.getOrderId())) {
        logger.warn("Order already processed: {}", event.getOrderId());
        return;
    }
    // Process order
    orderService.createOrder(event);
}
```

#### 2. Error Handling
- **Dead Letter Queue (DLQ)**: Route failed messages
- **Retry Logic**: Exponential backoff
- **Circuit Breaker**: Prevent cascading failures

**Example:**
```java
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void handleOrderCreated(
    @Payload OrderCreatedEvent event,
    @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
    @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
    @Header(KafkaHeaders.OFFSET) long offset
) {
    try {
        inventoryService.reserveInventory(event);
    } catch (Exception e) {
        logger.error("Failed to process order: {}", event.getOrderId(), e);
        // Send to DLQ
        kafkaTemplate.send("order-created-dlq", event.getOrderId(), event);
    }
}
```

#### 3. Performance Optimization
- **Batching**: Batch consumer records
- **Parallelism**: Use partitions effectively
- **State Management**: Optimize state stores

**Example:**
```yaml
spring:
  kafka:
    consumer:
      max-poll-records: 500  # Batch size
      fetch-min-size: 1MB    # Minimum fetch size
    listener:
      concurrency: 3         # Number of consumer threads
```

#### 4. Monitoring
- **Lag Monitoring**: Track consumer lag
- **Throughput Metrics**: Monitor messages/second
- **Error Rates**: Track failure rates

### Bulk Processing Best Practices

#### 1. Incremental Processing
- Process only new/changed data
- Use timestamps or change data capture (CDC)

**Example:**
```java
// Process only orders from last hour
LocalDateTime lastProcessed = getLastProcessedTime();
List<Order> newOrders = orderRepository
    .findByCreatedAtAfter(lastProcessed);

processOrders(newOrders);
updateLastProcessedTime(LocalDateTime.now());
```

#### 2. Partitioning
- Partition by date, region, or key
- Process partitions in parallel

**Example:**
```java
// Process orders by date partition
LocalDate startDate = LocalDate.now().minusDays(7);
LocalDate endDate = LocalDate.now();

for (LocalDate date = startDate; 
     date.isBefore(endDate); 
     date = date.plusDays(1)) {
    processOrdersForDate(date);
}
```

#### 3. Resource Management
- Use connection pooling
- Batch database operations
- Optimize memory usage

**Example:**
```java
// Batch insert
@Transactional
public void bulkInsertOrders(List<Order> orders) {
    int batchSize = 1000;
    for (int i = 0; i < orders.size(); i += batchSize) {
        List<Order> batch = orders.subList(
            i, Math.min(i + batchSize, orders.size())
        );
        orderRepository.saveAll(batch);
        orderRepository.flush();
        orderRepository.clear(); // Clear persistence context
    }
}
```

#### 4. Idempotency
- Make jobs idempotent
- Use checkpoints
- Handle failures gracefully

**Example:**
```java
// Idempotent batch job
public void processDailyOrders(LocalDate date) {
    String jobId = "daily-orders-" + date;
    
    // Check if already processed
    if (jobRepository.existsByJobId(jobId)) {
        logger.info("Job already completed: {}", jobId);
        return;
    }
    
    try {
        processOrders(date);
        jobRepository.save(new JobExecution(jobId, Status.COMPLETED));
    } catch (Exception e) {
        jobRepository.save(new JobExecution(jobId, Status.FAILED));
        throw e;
    }
}
```

---

## Strategies and Patterns

### 1. Lambda Architecture
Combine stream and batch processing:
- **Speed Layer**: Real-time processing (stream)
- **Batch Layer**: Accurate processing (batch)
- **Serving Layer**: Merge results

**Example:**
```
Real-time: Kafka Streams â†’ Redis (fast, approximate)
Batch: Spark â†’ Data Warehouse (accurate, complete)
Serving: Merge both for queries
```

### 2. Kappa Architecture
Use only stream processing:
- Single stream processing pipeline
- Reprocess historical data when needed
- Simpler than Lambda

**Example:**
```
All data â†’ Kafka â†’ Stream Processor â†’ Results
(Reprocess from Kafka when logic changes)
```

### 3. Event Sourcing
Store all events as source of truth:
- Replay events to rebuild state
- Time travel queries
- Audit trail

**Example:**
```java
// Store events
eventStore.save(new OrderCreatedEvent(orderId, ...));
eventStore.save(new PaymentProcessedEvent(orderId, ...));
eventStore.save(new OrderShippedEvent(orderId, ...));

// Rebuild state
Order order = eventStore.replay(orderId);
```

### 4. CQRS (Command Query Responsibility Segregation)
Separate read and write models:
- **Write**: Optimized for writes (normalized)
- **Read**: Optimized for reads (denormalized)
- **Sync**: Stream processing keeps them in sync

**Example:**
```
Write Model: Order (normalized)
    â†“ (Kafka events)
Read Model: OrderView (denormalized with customer, product info)
```

### 5. Change Data Capture (CDC)
Capture database changes as events:
- Monitor database logs
- Publish changes to stream
- Enable real-time sync

**Example:**
```
Database â†’ Debezium â†’ Kafka â†’ Downstream Services
```

---

## Downstream Data Usage

### Benefits of Downstream Data Processing

#### 1. Real-Time Analytics
**Benefit:** Immediate insights from data

**Example:**
```java
// Real-time order analytics
@KafkaListener(topics = "order-created")
public void updateAnalytics(OrderCreatedEvent event) {
    // Update real-time dashboard
    analyticsService.incrementOrderCount();
    analyticsService.addToRevenue(event.getTotalAmount());
    analyticsService.updateTopProducts(event.getItems());
}
```

**Use Cases:**
- Live dashboards
- Real-time KPIs
- Monitoring alerts

#### 2. Data Warehousing
**Benefit:** Centralized data for analytics

**Example:**
```
Kafka â†’ Spark Streaming â†’ Snowflake
```

**Use Cases:**
- Business intelligence
- Historical analysis
- Reporting

#### 3. Search Indexing
**Benefit:** Real-time search updates

**Example:**
```java
@KafkaListener(topics = "product-updated")
public void updateSearchIndex(ProductUpdatedEvent event) {
    searchService.indexProduct(event.getProduct());
}
```

**Use Cases:**
- Product search
- Full-text search
- Recommendation engines

#### 4. Cache Invalidation
**Benefit:** Keep caches fresh

**Example:**
```java
@KafkaListener(topics = "product-updated")
public void invalidateCache(ProductUpdatedEvent event) {
    cacheService.evict("product:" + event.getProductId());
}
```

**Use Cases:**
- Redis cache updates
- CDN cache invalidation
- Application cache sync

#### 5. Microservices Synchronization
**Benefit:** Keep services in sync

**Example:**
```
Order Service â†’ Kafka â†’ Inventory Service
Order Service â†’ Kafka â†’ Payment Service
Order Service â†’ Kafka â†’ Notification Service
```

**Use Cases:**
- Event-driven architecture
- Service decoupling
- Data consistency

#### 6. Machine Learning Features
**Benefit:** Real-time feature updates

**Example:**
```java
@KafkaListener(topics = "user-activity")
public void updateMLFeatures(UserActivityEvent event) {
    mlService.updateUserFeatures(event.getUserId(), event);
}
```

**Use Cases:**
- Real-time recommendations
- Fraud detection
- Personalization

#### 7. Audit and Compliance
**Benefit:** Complete audit trail

**Example:**
```
All events â†’ Kafka â†’ Audit Service â†’ Data Lake
```

**Use Cases:**
- Regulatory compliance
- Security auditing
- Data lineage

---

## Real-World Examples

### Example 1: E-Commerce Order Processing

#### Stream Processing Pipeline
```
Order Created â†’ Kafka â†’ Multiple Consumers
    â”œâ”€ Inventory Service: Reserve inventory
    â”œâ”€ Payment Service: Process payment
    â”œâ”€ Analytics Service: Update metrics
    â””â”€ Notification Service: Send confirmation
```

**Implementation:**
```java
// Order Service (Producer)
@Service
public class OrderService {
    @Autowired
    private OrderEventPublisher eventPublisher;
    
    public Order createOrder(CreateOrderRequest request) {
        Order order = orderRepository.save(new Order(request));
        
        // Publish event
        eventPublisher.publishOrderCreated(
            order.getId(),
            order.getBuyerId(),
            order.getItems(),
            order.getTotalAmount(),
            order.getShippingAddress()
        );
        
        return order;
    }
}

// Inventory Service (Consumer)
@Service
public class InventoryService {
    @KafkaListener(topics = "order-created", groupId = "inventory-group")
    public void reserveInventory(OrderCreatedEvent event) {
        for (OrderItem item : event.getItems()) {
            inventoryRepository.reserve(
                item.getProductId(),
                item.getQuantity()
            );
        }
        
        eventPublisher.publishInventoryReserved(
            event.getOrderId(),
            true
        );
    }
}
```

#### Bulk Processing Pipeline
```
Daily: Extract orders â†’ Transform â†’ Load to warehouse
```

**Implementation:**
```java
// Daily ETL Job
@Scheduled(cron = "0 0 2 * * *") // 2 AM daily
public void dailyOrderETL() {
    LocalDate yesterday = LocalDate.now().minusDays(1);
    
    // Extract
    List<Order> orders = orderRepository
        .findByCreatedAtBetween(
            yesterday.atStartOfDay(),
            yesterday.atTime(23, 59, 59)
        );
    
    // Transform
    List<OrderAnalytics> analytics = orders.stream()
        .map(this::transformToAnalytics)
        .collect(Collectors.toList());
    
    // Load
    analyticsRepository.saveAll(analytics);
}
```

### Example 2: Real-Time Fraud Detection with Apache Flink

**Scenario:** Detect fraudulent transactions in real-time

```java
// Flink Stream Processing
DataStream<Transaction> transactions = env
    .addSource(new KafkaSource<>("transactions"));

// Detect fraud patterns
DataStream<FraudAlert> alerts = transactions
    .keyBy(Transaction::getCustomerId)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
    .process(new FraudDetectionProcessFunction())
    .filter(alert -> alert.getRiskScore() > 0.8);

// Send alerts
alerts.addSink(new AlertSink());
```

**Fraud Detection Logic:**
```java
public class FraudDetectionProcessFunction 
    extends ProcessWindowFunction<Transaction, FraudAlert, Long, TimeWindow> {
    
    @Override
    public void process(Long customerId, 
                       Context context,
                       Iterable<Transaction> transactions,
                       Collector<FraudAlert> out) {
        
        List<Transaction> txList = new ArrayList<>();
        transactions.forEach(txList::add);
        
        // Check patterns
        double riskScore = 0.0;
        
        // Multiple transactions in short time
        if (txList.size() > 10) {
            riskScore += 0.3;
        }
        
        // High value transactions
        double totalAmount = txList.stream()
            .mapToDouble(Transaction::getAmount)
            .sum();
        if (totalAmount > 10000) {
            riskScore += 0.4;
        }
        
        // Unusual locations
        if (hasUnusualLocations(txList)) {
            riskScore += 0.3;
        }
        
        if (riskScore > 0.8) {
            out.collect(new FraudAlert(customerId, riskScore, txList));
        }
    }
}
```

### Example 3: Batch Analytics with Apache Spark

**Scenario:** Daily sales analytics and reporting

```scala
// Spark Batch Job
val spark = SparkSession.builder()
    .appName("DailySalesAnalytics")
    .getOrCreate()

// Read orders from data lake
val orders = spark.read
    .parquet("s3://data-lake/orders/date=2024-01-15/")

// Transform and aggregate
val dailySales = orders
    .filter($"status" === "COMPLETED")
    .groupBy($"product_id", $"category")
    .agg(
        sum($"amount").alias("total_revenue"),
        count("*").alias("order_count"),
        avg($"amount").alias("avg_order_value"),
        max($"amount").alias("max_order_value")
    )
    .withColumn("date", lit("2024-01-15"))

// Write to data warehouse
dailySales.write
    .mode("overwrite")
    .option("mergeSchema", "true")
    .parquet("s3://data-warehouse/daily-sales/")

// Generate report
val report = dailySales
    .orderBy($"total_revenue".desc)
    .limit(100)
    
report.coalesce(1)
    .write
    .mode("overwrite")
    .option("header", "true")
    .csv("s3://reports/top-products-2024-01-15.csv")
```

### Example 4: Snowflake Data Pipeline

**Scenario:** ELT pipeline from Kafka to Snowflake

```sql
-- Create stage for Kafka data
CREATE STAGE kafka_stage
URL = 's3://data-lake/kafka-data/'
CREDENTIALS = (AWS_KEY_ID='...' AWS_SECRET_KEY='...');

-- Create pipe for continuous loading
CREATE PIPE order_pipe
AUTO_INGEST = TRUE
AS
COPY INTO orders
FROM @kafka_stage/orders/
FILE_FORMAT = (TYPE = 'JSON');

-- Transform and create analytics table
CREATE OR REPLACE TABLE daily_order_analytics AS
SELECT 
    DATE(created_at) as order_date,
    product_id,
    category,
    SUM(amount) as total_revenue,
    COUNT(*) as order_count,
    AVG(amount) as avg_order_value,
    COUNT(DISTINCT customer_id) as unique_customers
FROM orders
WHERE created_at >= CURRENT_DATE() - 7
GROUP BY DATE(created_at), product_id, category;

-- Create materialized view for fast queries
CREATE MATERIALIZED VIEW mv_top_products AS
SELECT 
    product_id,
    SUM(total_revenue) as total_revenue_7d,
    SUM(order_count) as total_orders_7d
FROM daily_order_analytics
WHERE order_date >= CURRENT_DATE() - 7
GROUP BY product_id
ORDER BY total_revenue_7d DESC
LIMIT 100;
```

### Example 5: Real-Time Recommendation Engine

**Scenario:** Update recommendations based on user activity

```java
// Stream Processing: Update user preferences in real-time
@KafkaListener(topics = "user-activity", groupId = "recommendation-group")
public void updateRecommendations(UserActivityEvent event) {
    // Update user profile
    UserProfile profile = userProfileService.getProfile(event.getUserId());
    
    switch (event.getActivityType()) {
        case VIEW_PRODUCT:
            profile.addInterest(event.getProductId(), 1);
            break;
        case ADD_TO_CART:
            profile.addInterest(event.getProductId(), 3);
            break;
        case PURCHASE:
            profile.addInterest(event.getProductId(), 5);
            break;
    }
    
    userProfileService.updateProfile(profile);
    
    // Trigger recommendation update
    recommendationService.updateRecommendations(event.getUserId());
}

// Batch Processing: Train ML model daily
@Scheduled(cron = "0 0 3 * * *") // 3 AM daily
public void trainRecommendationModel() {
    // Extract user profiles and interactions
    List<UserProfile> profiles = userProfileService.getAllProfiles();
    List<Interaction> interactions = interactionRepository.findAll();
    
    // Train collaborative filtering model
    RecommendationModel model = mlService.trainModel(profiles, interactions);
    
    // Update model
    recommendationService.updateModel(model);
}
```

---

## Interview Questions

### Stream Processing Questions

#### 1. What is the difference between stream processing and batch processing?
**Answer:**
- **Stream processing**: Real-time, continuous processing of data as it arrives (milliseconds to seconds latency)
- **Batch processing**: Scheduled processing of discrete batches (minutes to hours latency)
- **Use cases**: Stream for real-time monitoring, fraud detection; Batch for ETL, reporting
- **Trade-offs**: Stream has lower latency but higher complexity; Batch is simpler but higher latency

#### 2. How does Kafka ensure message ordering?
**Answer:**
- Messages are ordered within a partition
- Use the same key to ensure related messages go to the same partition
- Multiple partitions allow parallel processing while maintaining order per partition
- Example: All events for orderId="123" go to partition 0, maintaining order

#### 3. What is consumer lag and how do you handle it?
**Answer:**
- **Consumer lag**: Difference between latest message offset and consumer's current offset
- **Causes**: Slow processing, insufficient consumers, network issues
- **Handling**:
  - Increase consumer instances
  - Optimize processing logic
  - Increase partition count
  - Use parallel processing
  - Monitor lag metrics

#### 4. Explain exactly-once semantics in Kafka.
**Answer:**
- **At-least-once**: Messages may be duplicated (default)
- **At-most-once**: Messages may be lost
- **Exactly-once**: Each message processed exactly once
- **Implementation**:
  - Idempotent producers (enable.idempotence=true)
  - Transactional producers
  - Idempotent consumers (check if already processed)

#### 5. How do you handle backpressure in stream processing?
**Answer:**
- **Backpressure**: Downstream can't keep up with upstream
- **Solutions**:
  - Buffering with bounded queues
  - Dropping messages (if acceptable)
  - Slowing down producers
  - Scaling consumers
  - Using backpressure-aware frameworks (Flink, Akka Streams)

#### 6. What is windowing in stream processing?
**Answer:**
- Grouping events by time or count
- **Types**:
  - **Tumbling**: Fixed, non-overlapping windows (e.g., 5-minute windows)
  - **Sliding**: Overlapping windows (e.g., 5-minute window, slide by 1 minute)
  - **Session**: Windows based on activity gaps
- **Use cases**: Aggregations, calculations over time periods

#### 7. How do you handle state in stream processing?
**Answer:**
- **State types**:
  - **Keyed state**: Per-key state (e.g., per customer)
  - **Operator state**: Operator-level state
  - **Window state**: State for windows
- **Storage**: In-memory, RocksDB, external stores (Redis, database)
- **Fault tolerance**: Checkpointing, state snapshots

### Bulk Processing Questions

#### 8. What is ETL and when do you use it?
**Answer:**
- **ETL**: Extract, Transform, Load
- **Extract**: Read from sources (databases, files, APIs)
- **Transform**: Clean, enrich, aggregate data
- **Load**: Write to destination (data warehouse, database)
- **Use cases**: Data warehousing, reporting, analytics
- **Alternatives**: ELT (Extract, Load, Transform) - load first, transform in destination

#### 9. How do you optimize batch processing performance?
**Answer:**
- **Partitioning**: Process data in parallel partitions
- **Incremental processing**: Process only new/changed data
- **Resource optimization**: Right-sizing compute, memory
- **Parallelism**: Process multiple partitions simultaneously
- **Caching**: Cache frequently accessed data
- **Compression**: Compress data for I/O efficiency

#### 10. What is the difference between full load and incremental load?
**Answer:**
- **Full load**: Process entire dataset (simpler, but slower)
- **Incremental load**: Process only new/changed data (faster, but more complex)
- **When to use**:
  - Full load: Initial load, small datasets, when changes are extensive
  - Incremental: Large datasets, frequent updates, cost optimization

#### 11. How do you ensure data quality in batch processing?
**Answer:**
- **Validation**: Schema validation, data type checks
- **Cleansing**: Remove duplicates, handle nulls, standardize formats
- **Monitoring**: Track data quality metrics
- **Testing**: Unit tests, integration tests
- **Error handling**: Log errors, send to DLQ, retry logic

#### 12. Explain Lambda Architecture.
**Answer:**
- Combines stream and batch processing
- **Speed layer**: Real-time processing (low latency, approximate)
- **Batch layer**: Accurate processing (high latency, precise)
- **Serving layer**: Merges both for queries
- **Use case**: When you need both real-time and accurate results
- **Alternative**: Kappa Architecture (stream-only, reprocess when needed)

### Technology-Specific Questions

#### 13. When would you use Apache Spark vs Apache Flink?
**Answer:**
- **Spark**:
  - Batch processing (excellent)
  - Micro-batch streaming
  - Large-scale data processing
  - ML/AI workloads
  - Use when: Batch jobs, ETL, analytics
  
- **Flink**:
  - True stream processing (low latency)
  - Event time processing
  - Complex event processing
  - Use when: Real-time requirements, low latency, event-driven

#### 14. What are the benefits of Snowflake?
**Answer:**
- **Separation of storage and compute**: Scale independently
- **Automatic scaling**: Auto-scale based on workload
- **Time travel**: Query historical data
- **Zero-copy cloning**: Instant clones for testing
- **Multi-cloud**: Works on AWS, Azure, GCP
- **Performance**: Columnar storage, automatic optimization

#### 15. How does Kafka Streams differ from Kafka Consumers?
**Answer:**
- **Kafka Consumers**: Simple message consumption
- **Kafka Streams**: Stream processing library built on Kafka
  - Stateful processing
  - Windowing and aggregations
  - Joins between streams
  - Exactly-once semantics
  - DSL for stream processing
- **Use Kafka Streams when**: Need stream processing without external framework

---

## Summary

### Key Takeaways

1. **Stream Processing**: Use for real-time requirements, continuous data, event-driven actions
2. **Bulk Processing**: Use for scheduled jobs, large volumes, complex transformations
3. **Hybrid Approach**: Lambda or Kappa architecture for comprehensive solutions
4. **Technology Choice**: Depends on latency requirements, data volume, and use case
5. **Best Practices**: Idempotency, error handling, monitoring, optimization

### Decision Matrix

| Requirement | Recommended Approach |
|------------|---------------------|
| Real-time fraud detection | Stream (Flink, Kafka Streams) |
| Daily sales reports | Bulk (Spark, Airflow) |
| Real-time dashboards | Stream (Kafka, Flink) |
| Data warehouse ETL | Bulk (Spark, Airflow) |
| Real-time recommendations | Stream + Bulk (Hybrid) |
| Historical analysis | Bulk (Spark, Snowflake) |

---

## Advanced Topics

### Performance Tuning

#### Stream Processing Performance

**1. Kafka Producer Tuning**
```yaml
spring:
  kafka:
    producer:
      # Acknowledgment: all = wait for all replicas (most reliable)
      acks: all
      # Retries for transient failures
      retries: 3
      # Idempotent producer (exactly-once semantics)
      enable-idempotence: true
      # Batch size for better throughput
      batch-size: 16384  # 16KB
      # Wait time before sending batch
      linger-ms: 10
      # Compression for network efficiency
      compression-type: snappy
      # Buffer memory
      buffer-memory: 33554432  # 32MB
      # Max in-flight requests (1 for exactly-once)
      max-in-flight-requests-per-connection: 1
```

**2. Kafka Consumer Tuning**
```yaml
spring:
  kafka:
    consumer:
      # Fetch size
      fetch-min-size: 1MB
      fetch-max-wait: 500ms
      # Max records per poll
      max-poll-records: 500
      # Session timeout
      session-timeout-ms: 30000
      # Heartbeat interval
      heartbeat-interval-ms: 3000
      # Auto commit (disable for manual control)
      enable-auto-commit: false
    listener:
      # Concurrency (number of consumer threads)
      concurrency: 3
      # Ack mode
      ack-mode: manual_immediate
      # Poll timeout
      poll-timeout: 3000
```

**3. Parallelism Optimization**
- **Partitions**: More partitions = more parallelism
- **Consumer Instances**: Match consumer instances to partitions
- **Thread Pool**: Use thread pools for async processing

**Example:**
```java
@KafkaListener(
    topics = "order-created",
    groupId = "inventory-group",
    concurrency = "3"  // 3 consumer threads
)
public void processOrder(OrderCreatedEvent event) {
    // Process order
}
```

#### Bulk Processing Performance

**1. Spark Tuning**
```scala
val spark = SparkSession.builder()
    .appName("BatchProcessing")
    .config("spark.sql.shuffle.partitions", "200")  // Parallelism
    .config("spark.executor.memory", "4g")          // Memory
    .config("spark.executor.cores", "4")            // Cores
    .config("spark.sql.adaptive.enabled", "true")   // Adaptive execution
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
    .getOrCreate()
```

**2. Database Batch Operations**
```java
// JPA Batch Insert
@Configuration
public class JpaConfig {
    @Bean
    public LocalContainerEntityManagerFactoryBean entityManagerFactory() {
        LocalContainerEntityManagerFactoryBean em = new LocalContainerEntityManagerFactoryBean();
        Properties props = new Properties();
        props.put("hibernate.jdbc.batch_size", "1000");
        props.put("hibernate.order_inserts", "true");
        props.put("hibernate.order_updates", "true");
        props.put("hibernate.jdbc.batch_versioned_data", "true");
        em.setJpaProperties(props);
        return em;
    }
}

// Batch Insert Example
@Transactional
public void bulkInsert(List<Order> orders) {
    int batchSize = 1000;
    for (int i = 0; i < orders.size(); i += batchSize) {
        List<Order> batch = orders.subList(
            i, Math.min(i + batchSize, orders.size())
        );
        orderRepository.saveAll(batch);
        orderRepository.flush();
        entityManager.clear();  // Clear persistence context
    }
}
```

### Monitoring and Observability

#### Stream Processing Metrics

**1. Kafka Metrics**
- **Producer Metrics**:
  - `records-sent-rate`: Messages sent per second
  - `record-error-rate`: Error rate
  - `request-latency-avg`: Average latency
  - `batch-size-avg`: Average batch size

- **Consumer Metrics**:
  - `records-consumed-rate`: Messages consumed per second
  - `records-lag`: Consumer lag
  - `fetch-latency-avg`: Fetch latency
  - `commit-latency-avg`: Commit latency

**2. Custom Metrics**
```java
@Service
public class OrderProcessingMetrics {
    private final MeterRegistry meterRegistry;
    private final Counter ordersProcessed;
    private final Timer processingTime;
    
    public OrderProcessingMetrics(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        this.ordersProcessed = Counter.builder("orders.processed")
            .description("Number of orders processed")
            .register(meterRegistry);
        this.processingTime = Timer.builder("orders.processing.time")
            .description("Order processing time")
            .register(meterRegistry);
    }
    
    @KafkaListener(topics = "order-created")
    public void processOrder(OrderCreatedEvent event) {
        Timer.Sample sample = Timer.start(meterRegistry);
        try {
            // Process order
            orderService.processOrder(event);
            ordersProcessed.increment();
        } finally {
            sample.stop(processingTime);
        }
    }
}
```

**3. Consumer Lag Monitoring**
```java
@Component
public class ConsumerLagMonitor {
    @Autowired
    private KafkaConsumer<String, Object> kafkaConsumer;
    
    @Scheduled(fixedRate = 60000)  // Every minute
    public void checkConsumerLag() {
        Map<TopicPartition, Long> endOffsets = kafkaConsumer.endOffsets(
            kafkaConsumer.assignment()
        );
        
        for (TopicPartition partition : kafkaConsumer.assignment()) {
            long currentOffset = kafkaConsumer.position(partition);
            long endOffset = endOffsets.get(partition);
            long lag = endOffset - currentOffset;
            
            if (lag > 10000) {  // Alert if lag > 10k
                logger.warn("High consumer lag detected: partition={}, lag={}", 
                    partition, lag);
            }
        }
    }
}
```

#### Bulk Processing Metrics

**1. Job Execution Metrics**
```java
@Component
public class BatchJobMetrics {
    private final MeterRegistry meterRegistry;
    
    public void recordJobExecution(String jobName, Duration duration, boolean success) {
        Timer.builder("batch.job.execution.time")
            .tag("job", jobName)
            .tag("status", success ? "success" : "failure")
            .register(meterRegistry)
            .record(duration);
        
        Counter.builder("batch.job.execution.count")
            .tag("job", jobName)
            .tag("status", success ? "success" : "failure")
            .register(meterRegistry)
            .increment();
    }
    
    public void recordRecordsProcessed(String jobName, long count) {
        Counter.builder("batch.records.processed")
            .tag("job", jobName)
            .register(meterRegistry)
            .increment(count);
    }
}
```

**2. Data Quality Metrics**
```java
public class DataQualityMetrics {
    private long totalRecords;
    private long validRecords;
    private long invalidRecords;
    private Map<String, Long> errorCounts = new HashMap<>();
    
    public void recordValidRecord() {
        totalRecords++;
        validRecords++;
    }
    
    public void recordInvalidRecord(String errorType) {
        totalRecords++;
        invalidRecords++;
        errorCounts.merge(errorType, 1L, Long::sum);
    }
    
    public double getQualityScore() {
        return totalRecords > 0 ? (double) validRecords / totalRecords : 0.0;
    }
}
```

### Error Handling Strategies

#### Stream Processing Error Handling

**1. Dead Letter Queue (DLQ)**
```java
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void processOrder(
    @Payload OrderCreatedEvent event,
    @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
    @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
    @Header(KafkaHeaders.OFFSET) long offset
) {
    try {
        inventoryService.reserveInventory(event);
    } catch (TransientException e) {
        // Retry transient errors
        logger.warn("Transient error, will retry: orderId={}", event.getOrderId(), e);
        throw e;  // Kafka will retry
    } catch (PermanentException e) {
        // Send to DLQ for permanent errors
        logger.error("Permanent error, sending to DLQ: orderId={}", 
            event.getOrderId(), e);
        sendToDLQ(topic, partition, offset, event, e);
    } catch (Exception e) {
        // Unknown errors
        logger.error("Unknown error: orderId={}", event.getOrderId(), e);
        sendToDLQ(topic, partition, offset, event, e);
    }
}

private void sendToDLQ(String topic, int partition, long offset, 
                      OrderCreatedEvent event, Exception error) {
    DLQMessage dlqMessage = new DLQMessage(
        topic, partition, offset, event, error.getMessage(), LocalDateTime.now()
    );
    kafkaTemplate.send("order-created-dlq", event.getOrderId(), dlqMessage);
}
```

**2. Retry with Exponential Backoff**
```java
@Component
public class RetryableKafkaListener {
    private static final int MAX_RETRIES = 3;
    private static final long INITIAL_DELAY = 1000;  // 1 second
    
    @KafkaListener(topics = "order-created", groupId = "inventory-group")
    public void processOrder(OrderCreatedEvent event) {
        int retries = 0;
        while (retries < MAX_RETRIES) {
            try {
                inventoryService.reserveInventory(event);
                return;  // Success
            } catch (Exception e) {
                retries++;
                if (retries >= MAX_RETRIES) {
                    logger.error("Max retries exceeded: orderId={}", 
                        event.getOrderId(), e);
                    sendToDLQ(event, e);
                    return;
                }
                
                long delay = INITIAL_DELAY * (long) Math.pow(2, retries - 1);
                logger.warn("Retry {} after {}ms: orderId={}", 
                    retries, delay, event.getOrderId());
                try {
                    Thread.sleep(delay);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    return;
                }
            }
        }
    }
}
```

**3. Circuit Breaker Pattern**
```java
@Service
public class ResilientInventoryService {
    private final CircuitBreaker circuitBreaker;
    
    public ResilientInventoryService() {
        this.circuitBreaker = CircuitBreaker.of("inventory-service",
            CircuitBreakerConfig.custom()
                .failureRateThreshold(50)
                .waitDurationInOpenState(Duration.ofSeconds(30))
                .slidingWindowSize(10)
                .build()
        );
    }
    
    @KafkaListener(topics = "order-created", groupId = "inventory-group")
    public void processOrder(OrderCreatedEvent event) {
        Try.ofSupplier(CircuitBreaker.decorateSupplier(circuitBreaker, () -> {
            return inventoryService.reserveInventory(event);
        }))
        .onFailure(throwable -> {
            logger.error("Circuit breaker failure: orderId={}", 
                event.getOrderId(), throwable);
            sendToDLQ(event, throwable);
        })
        .onSuccess(result -> {
            logger.info("Inventory reserved: orderId={}", event.getOrderId());
        });
    }
}
```

#### Bulk Processing Error Handling

**1. Checkpointing**
```java
@Service
public class CheckpointedBatchJob {
    @Autowired
    private JobCheckpointRepository checkpointRepository;
    
    public void processOrders(LocalDate date) {
        String jobId = "daily-orders-" + date;
        JobCheckpoint checkpoint = checkpointRepository.findByJobId(jobId)
            .orElse(new JobCheckpoint(jobId, 0L));
        
        long lastProcessedId = checkpoint.getLastProcessedId();
        int batchSize = 1000;
        
        while (true) {
            List<Order> orders = orderRepository
                .findByIdGreaterThan(lastProcessedId, PageRequest.of(0, batchSize));
            
            if (orders.isEmpty()) {
                break;
            }
            
            try {
                processBatch(orders);
                lastProcessedId = orders.get(orders.size() - 1).getId();
                checkpoint.setLastProcessedId(lastProcessedId);
                checkpointRepository.save(checkpoint);
            } catch (Exception e) {
                logger.error("Error processing batch, checkpoint saved: lastId={}", 
                    lastProcessedId, e);
                throw e;  // Job can resume from checkpoint
            }
        }
    }
}
```

**2. Partial Failure Handling**
```java
public void processOrdersWithPartialFailure(List<Order> orders) {
    List<Order> failedOrders = new ArrayList<>();
    
    for (Order order : orders) {
        try {
            processOrder(order);
        } catch (Exception e) {
            logger.error("Failed to process order: orderId={}", order.getId(), e);
            failedOrders.add(order);
        }
    }
    
    // Retry failed orders
    if (!failedOrders.isEmpty()) {
        logger.warn("Retrying {} failed orders", failedOrders.size());
        retryFailedOrders(failedOrders);
    }
    
    // Report final failures
    if (!failedOrders.isEmpty()) {
        sendFailureReport(failedOrders);
    }
}
```

### Data Consistency Patterns

#### 1. Exactly-Once Processing

**Kafka Exactly-Once:**
```yaml
spring:
  kafka:
    producer:
      enable-idempotence: true
      transaction-id-prefix: "prod-"
    consumer:
      isolation-level: read_committed
    listener:
      ack-mode: manual_immediate
```

**Implementation:**
```java
@KafkaListener(topics = "order-created", groupId = "inventory-group")
@Transactional
public void processOrder(OrderCreatedEvent event, Acknowledgment ack) {
    // Check idempotency
    if (orderRepository.existsById(event.getOrderId())) {
        logger.warn("Order already processed: {}", event.getOrderId());
        ack.acknowledge();
        return;
    }
    
    // Process order
    inventoryService.reserveInventory(event);
    
    // Acknowledge after successful processing
    ack.acknowledge();
}
```

#### 2. Outbox Pattern
Ensure database and message publishing are consistent:

```java
@Entity
public class OutboxEvent {
    @Id
    private String id;
    private String aggregateId;
    private String eventType;
    private String payload;
    private LocalDateTime createdAt;
    private boolean published;
}

@Service
public class OrderService {
    @Transactional
    public Order createOrder(CreateOrderRequest request) {
        // Save order
        Order order = orderRepository.save(new Order(request));
        
        // Save outbox event (same transaction)
        OutboxEvent outboxEvent = new OutboxEvent(
            UUID.randomUUID().toString(),
            order.getId(),
            "OrderCreated",
            objectMapper.writeValueAsString(new OrderCreatedEvent(order)),
            LocalDateTime.now(),
            false
        );
        outboxRepository.save(outboxEvent);
        
        return order;
    }
}

// Poller to publish outbox events
@Scheduled(fixedRate = 5000)
@Transactional
public void publishOutboxEvents() {
    List<OutboxEvent> events = outboxRepository
        .findByPublishedFalse(PageRequest.of(0, 100));
    
    for (OutboxEvent event : events) {
        try {
            kafkaTemplate.send(event.getEventType(), event.getAggregateId(), 
                event.getPayload());
            event.setPublished(true);
            outboxRepository.save(event);
        } catch (Exception e) {
            logger.error("Failed to publish outbox event: {}", event.getId(), e);
        }
    }
}
```

#### 3. Saga Pattern for Distributed Transactions
```java
// Choreography-based Saga
@KafkaListener(topics = "order-created", groupId = "payment-group")
public void processPayment(OrderCreatedEvent event) {
    try {
        paymentService.processPayment(event);
        eventPublisher.publishPaymentProcessed(
            event.getOrderId(), event.getBuyerId(), 
            event.getTotalAmount(), true
        );
    } catch (Exception e) {
        eventPublisher.publishPaymentProcessed(
            event.getOrderId(), event.getBuyerId(), 
            event.getTotalAmount(), false
        );
    }
}

@KafkaListener(topics = "payment-processed", groupId = "inventory-group")
public void handlePaymentResult(PaymentProcessedEvent event) {
    if (event.isSuccess()) {
        // Continue saga
        inventoryService.reserveInventory(event.getOrderId());
    } else {
        // Compensate: cancel order
        orderService.cancelOrder(event.getOrderId(), "Payment failed");
    }
}
```

### Scaling Strategies

#### Horizontal Scaling

**1. Kafka Partitioning**
- Increase partitions for more parallelism
- Rule: One consumer per partition maximum
- Rebalance when adding/removing consumers

**Example:**
```bash
# Increase partitions
kafka-topics --alter --topic order-created --partitions 10 \
  --bootstrap-server localhost:9092
```

**2. Consumer Scaling**
```yaml
# Scale consumers to match partitions
spring:
  kafka:
    consumer:
      group-id: inventory-group
    listener:
      concurrency: 10  # Match partition count
```

**3. Batch Processing Scaling**
- **Spark**: Increase executors, cores, memory
- **Airflow**: Scale workers
- **Snowflake**: Auto-scale warehouses

### Cost Optimization

#### Stream Processing Cost Optimization

**1. Message Compression**
```yaml
spring:
  kafka:
    producer:
      compression-type: snappy  # or gzip, lz4
```

**2. Retention Policies**
```bash
# Set retention to reduce storage
kafka-configs --alter --entity-type topics \
  --entity-name order-created \
  --add-config retention.ms=604800000  # 7 days
```

**3. Consumer Group Management**
- Use separate consumer groups for different use cases
- Avoid unnecessary consumers

#### Bulk Processing Cost Optimization

**1. Incremental Processing**
- Process only new/changed data
- Reduces compute costs

**2. Right-Sizing Resources**
- Use appropriate instance sizes
- Auto-scale based on workload

**3. Data Partitioning**
- Partition data to process only needed partitions
- Reduces data scanned

---

## Additional Interview Questions

### Advanced Stream Processing Questions

#### 16. How do you handle late-arriving events in stream processing?
**Answer:**
- **Watermarks**: Define how late events can be
- **Allowed Lateness**: Accept events within a grace period
- **Side Outputs**: Route late events to separate stream
- **Event Time vs Processing Time**: Use event time for accurate results

**Example (Flink):**
```java
stream.assignTimestampsAndWatermarks(
    WatermarkStrategy.<Event>forBoundedOutOfOrderness(
        Duration.ofSeconds(10)  // 10 second lateness allowed
    )
    .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
)
.window(TumblingEventTimeWindows.of(Time.minutes(5)))
.aggregate(new OrderAggregator());
```

#### 17. What is the difference between at-least-once and exactly-once processing?
**Answer:**
- **At-least-once**: Messages may be processed multiple times (duplicates possible)
- **Exactly-once**: Each message processed exactly once (no duplicates, no loss)
- **Trade-offs**: Exactly-once has higher overhead but guarantees correctness
- **Implementation**: Idempotent operations, transactions, deduplication

#### 18. How do you handle schema evolution in Kafka?
**Answer:**
- **Schema Registry**: Centralized schema management
- **Compatibility Modes**: Backward, forward, full compatibility
- **Versioning**: Schema versions for evolution
- **Best Practices**: 
  - Add optional fields (backward compatible)
  - Don't remove required fields
  - Use schema registry (Avro, Protobuf, JSON Schema)

**Example:**
```java
// Avro with Schema Registry
@Bean
public ProducerFactory<String, OrderEvent> producerFactory() {
    Map<String, Object> props = new HashMap<>();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
        KafkaAvroSerializer.class);
    props.put("schema.registry.url", "http://localhost:8081");
    return new DefaultKafkaProducerFactory<>(props);
}
```

#### 19. Explain the difference between Kafka Streams and Kafka Consumer API.
**Answer:**
- **Kafka Consumer API**: Low-level, manual offset management, simple consumption
- **Kafka Streams**: High-level DSL, automatic state management, stream processing operations
- **Use Consumer API when**: Simple consumption, custom logic
- **Use Kafka Streams when**: Need aggregations, joins, stateful processing

#### 20. How do you ensure ordering in distributed stream processing?
**Answer:**
- **Partition Key**: Same key â†’ same partition â†’ ordering
- **Single Partition**: All events in one partition (limits parallelism)
- **Key-based Partitioning**: Related events use same key
- **Trade-off**: Ordering vs parallelism

**Example:**
```java
// Ensure all events for an order are ordered
kafkaTemplate.send("order-events", orderId, event);  // orderId as key
```

### Advanced Bulk Processing Questions

#### 21. What is the difference between ETL and ELT?
**Answer:**
- **ETL**: Extract â†’ Transform â†’ Load (transform before loading)
- **ELT**: Extract â†’ Load â†’ Transform (load first, transform in destination)
- **ETL Use Cases**: When transformation is complex, source systems have limited compute
- **ELT Use Cases**: When destination has powerful compute (Snowflake, BigQuery), faster loading

#### 22. How do you handle data skew in batch processing?
**Answer:**
- **Problem**: Uneven data distribution causes some partitions to be slow
- **Solutions**:
  - **Salting**: Add random salt to keys
  - **Custom Partitioning**: Distribute data evenly
  - **Broadcast Joins**: For small tables
  - **Skew Join Optimization**: Handle skewed keys separately

**Example (Spark):**
```scala
// Salting to handle skew
val saltedOrders = orders
    .withColumn("salt", floor(rand() * 10))
    .withColumn("salted_key", concat($"order_id", lit("_"), $"salt"))

val result = saltedOrders
    .join(customers, $"salted_key" === $"customer_key")
    .groupBy($"order_id")
    .agg(sum($"amount"))
```

#### 23. Explain the difference between batch and micro-batch processing.
**Answer:**
- **Batch**: Process large chunks at scheduled intervals (hours, days)
- **Micro-batch**: Process small batches frequently (seconds, minutes)
- **Stream**: Process events individually as they arrive
- **Use Cases**:
  - Batch: Daily reports, ETL
  - Micro-batch: Near real-time analytics (Spark Streaming)
  - Stream: Real-time processing (Flink, Kafka Streams)

#### 24. How do you optimize Spark jobs?
**Answer:**
- **Partitioning**: Right number of partitions (2-3x cores)
- **Caching**: Cache frequently used DataFrames
- **Broadcast Variables**: For small lookup tables
- **Data Skew**: Handle skewed data
- **Join Optimization**: Use broadcast joins for small tables
- **Predicate Pushdown**: Filter early
- **Column Pruning**: Select only needed columns

**Example:**
```scala
// Optimize Spark job
val orders = spark.read.parquet("s3://bucket/orders/")
    .filter($"date" === currentDate())  // Predicate pushdown
    .select("order_id", "amount", "customer_id")  // Column pruning
    .cache()  // Cache for reuse

val customers = spark.read.parquet("s3://bucket/customers/")
    .broadcast()  // Broadcast small table

val result = orders.join(customers, "customer_id")
    .repartition(200)  // Right partitioning
```

#### 25. What is data lake vs data warehouse?
**Answer:**
- **Data Lake**: Raw data storage (files, various formats), schema-on-read, cost-effective
- **Data Warehouse**: Structured data, schema-on-write, optimized for queries
- **Use Cases**:
  - Data Lake: Big data, raw storage, exploration
  - Data Warehouse: Analytics, reporting, BI
- **Modern Approach**: Data Lakehouse (combines both)

### Architecture Questions

#### 26. When would you choose Lambda vs Kappa architecture?
**Answer:**
- **Lambda**: Need both real-time and batch processing, accuracy important
  - Speed layer (stream) + Batch layer + Serving layer
  - More complex, maintain two systems
- **Kappa**: Stream-only architecture, simpler
  - Single stream processing pipeline
  - Reprocess from stream when needed
  - Simpler, but requires stream to store all data

**Choose Lambda when**: Need both real-time and accurate results
**Choose Kappa when**: Can reprocess from stream, want simplicity

#### 27. How do you handle backpressure in stream processing?
**Answer:**
- **Problem**: Downstream can't keep up with upstream
- **Solutions**:
  - **Buffering**: Bounded buffers (may cause memory issues)
  - **Dropping**: Drop messages (if acceptable)
  - **Throttling**: Slow down producers
  - **Scaling**: Scale consumers
  - **Backpressure-aware frameworks**: Flink, Akka Streams handle automatically

#### 28. Explain event sourcing and CQRS.
**Answer:**
- **Event Sourcing**: Store all events as source of truth
  - Replay events to rebuild state
  - Complete audit trail
  - Time travel queries
- **CQRS**: Separate read and write models
  - Write model optimized for writes
  - Read model optimized for reads
  - Stream processing keeps them in sync
- **Together**: Event sourcing for writes, CQRS for reads

**Example:**
```
Write: Order Service â†’ Events â†’ Event Store
Read: Event Store â†’ Stream Processing â†’ Read Model (OrderView)
```

---

## Real-World Scenarios

### Scenario 1: E-Commerce Order Processing Pipeline

**Requirements:**
- Real-time order processing
- Daily sales analytics
- Inventory updates
- Customer notifications

**Architecture:**
```
Order Service â†’ Kafka â†’ Multiple Consumers
    â”œâ”€ Inventory Service (Stream)
    â”œâ”€ Payment Service (Stream)
    â”œâ”€ Analytics Service (Stream + Batch)
    â””â”€ Notification Service (Stream)

Daily: Spark â†’ Snowflake (Batch Analytics)
```

**Implementation:**
- **Stream**: Real-time order processing, inventory updates
- **Batch**: Daily sales reports, customer segmentation
- **Hybrid**: Real-time dashboards + accurate batch reports

### Scenario 2: IoT Sensor Data Processing

**Requirements:**
- Process millions of sensor readings per second
- Real-time anomaly detection
- Historical analysis
- Alert on thresholds

**Architecture:**
```
IoT Sensors â†’ Kafka â†’ Flink â†’ Multiple Sinks
    â”œâ”€ Real-time Alerts (Stream)
    â”œâ”€ Time Series DB (Stream)
    â””â”€ Data Lake (Stream)

Daily: Spark â†’ Data Warehouse (Batch Analytics)
```

**Implementation:**
- **Stream (Flink)**: Real-time processing, windowing, aggregations
- **Batch (Spark)**: Historical analysis, ML model training

### Scenario 3: Financial Trading System

**Requirements:**
- Ultra-low latency (< 1ms)
- Exactly-once processing
- Real-time risk calculations
- Regulatory reporting

**Architecture:**
```
Trading System â†’ Kafka â†’ Flink â†’ Risk Engine
    â”œâ”€ Real-time Risk (Stream)
    â”œâ”€ Trade Matching (Stream)
    â””â”€ Regulatory Reporting (Batch)
```

**Implementation:**
- **Stream (Flink)**: Real-time risk, trade matching
- **Batch**: Regulatory reports, reconciliation

---

## Troubleshooting

### Common Stream Processing Issues

#### 1. Consumer Lag
**Symptoms:**
- Consumers falling behind producers
- Increasing lag metrics
- Delayed processing

**Causes:**
- Slow processing logic
- Insufficient consumers
- Network issues
- Resource constraints

**Solutions:**
```java
// Increase consumer instances
spring:
  kafka:
    listener:
      concurrency: 5  # Match or exceed partition count

// Optimize processing
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void processOrder(OrderCreatedEvent event) {
    // Use async processing
    CompletableFuture.runAsync(() -> {
        inventoryService.reserveInventory(event);
    }, executorService);
}

// Monitor lag
@Scheduled(fixedRate = 60000)
public void monitorLag() {
    // Check consumer lag and alert if high
}
```

#### 2. Message Duplication
**Symptoms:**
- Same message processed multiple times
- Duplicate records in database

**Causes:**
- Consumer crashes after processing but before commit
- Retry logic without idempotency

**Solutions:**
```java
// Idempotent processing
@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void processOrder(OrderCreatedEvent event) {
    // Check if already processed
    if (orderRepository.existsById(event.getOrderId())) {
        logger.warn("Order already processed: {}", event.getOrderId());
        return;
    }
    
    // Process order
    inventoryService.reserveInventory(event);
}

// Use idempotent producer
spring:
  kafka:
    producer:
      enable-idempotence: true
```

#### 3. Message Loss
**Symptoms:**
- Messages not processed
- Missing records

**Causes:**
- Auto-commit before processing
- Consumer crashes
- Wrong offset reset policy

**Solutions:**
```java
// Manual acknowledgment
spring:
  kafka:
    consumer:
      enable-auto-commit: false
    listener:
      ack-mode: manual_immediate

@KafkaListener(topics = "order-created", groupId = "inventory-group")
public void processOrder(OrderCreatedEvent event, Acknowledgment ack) {
    try {
        inventoryService.reserveInventory(event);
        ack.acknowledge();  // Only after successful processing
    } catch (Exception e) {
        // Don't acknowledge - will retry
    }
}
```

#### 4. Partition Rebalancing
**Symptoms:**
- Temporary processing delays
- Consumer group rebalancing

**Causes:**
- Adding/removing consumers
- Consumer timeouts
- Network issues

**Solutions:**
```yaml
# Increase session timeout
spring:
  kafka:
    consumer:
      session-timeout-ms: 30000
      heartbeat-interval-ms: 3000
      max-poll-interval-ms: 300000  # 5 minutes
```

### Common Bulk Processing Issues

#### 1. Out of Memory
**Symptoms:**
- OOM errors
- JVM crashes
- Slow processing

**Causes:**
- Loading too much data into memory
- No batching
- Memory leaks

**Solutions:**
```java
// Process in batches
public void processOrders(LocalDate date) {
    int batchSize = 1000;
    int offset = 0;
    
    while (true) {
        List<Order> batch = orderRepository
            .findByDate(date, PageRequest.of(offset, batchSize));
        
        if (batch.isEmpty()) break;
        
        processBatch(batch);
        entityManager.clear();  // Clear persistence context
        offset += batchSize;
    }
}

// Increase JVM memory
// -Xmx4g -Xms2g
```

#### 2. Slow Performance
**Symptoms:**
- Long-running jobs
- Timeouts

**Causes:**
- No parallelism
- Inefficient queries
- No indexing
- Network latency

**Solutions:**
```java
// Parallel processing
List<LocalDate> dates = getDateRange(startDate, endDate);
dates.parallelStream().forEach(date -> {
    processOrdersForDate(date);
});

// Optimize queries
@Query("SELECT o FROM Order o WHERE o.date = :date AND o.status = :status")
List<Order> findOrdersByDateAndStatus(@Param("date") LocalDate date, 
                                       @Param("status") OrderStatus status);

// Add indexes
@Table(indexes = @Index(name = "idx_order_date_status", 
                       columnList = "date, status"))
```

#### 3. Data Quality Issues
**Symptoms:**
- Invalid data
- Missing fields
- Format errors

**Causes:**
- No validation
- Schema mismatches
- Data corruption

**Solutions:**
```java
// Validate data
public void processOrder(Order order) {
    validateOrder(order);  // Throw exception if invalid
    
    // Process valid order
    orderService.save(order);
}

private void validateOrder(Order order) {
    if (order.getAmount() == null || order.getAmount().compareTo(BigDecimal.ZERO) <= 0) {
        throw new ValidationException("Invalid order amount");
    }
    if (order.getCustomerId() == null) {
        throw new ValidationException("Customer ID is required");
    }
}

// Data quality metrics
public class DataQualityReport {
    private long totalRecords;
    private long validRecords;
    private long invalidRecords;
    private List<String> errors = new ArrayList<>();
}
```

### Debugging Tips

#### 1. Enable Debug Logging
```yaml
logging:
  level:
    org.apache.kafka: DEBUG
    org.springframework.kafka: DEBUG
    com.ekart: DEBUG
```

#### 2. Monitor Metrics
```java
// Expose Kafka metrics
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,kafka
  metrics:
    export:
      prometheus:
        enabled: true
```

#### 3. Use Kafka Tools
```bash
# List consumer groups
kafka-consumer-groups --bootstrap-server localhost:9092 --list

# Check consumer lag
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group inventory-group --describe

# View topic messages
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic order-created --from-beginning
```

---

## Conclusion

### Key Takeaways

1. **Choose the right approach**: Stream for real-time, batch for scheduled/analytics
2. **Hybrid is powerful**: Combine stream and batch for comprehensive solutions
3. **Technology matters**: Choose based on latency, volume, and use case
4. **Best practices**: Idempotency, error handling, monitoring, optimization
5. **Scalability**: Design for scale from the start

### Decision Framework

```
Is real-time required? (< 1 second)
â”œâ”€ Yes â†’ Stream Processing (Kafka, Flink, Kafka Streams)
â””â”€ No â†’ Is scheduled processing OK?
    â”œâ”€ Yes â†’ Batch Processing (Spark, Airflow)
    â””â”€ Need both? â†’ Hybrid (Lambda/Kappa Architecture)
```

### Next Steps

1. **Start Simple**: Begin with basic stream or batch processing
2. **Monitor**: Add monitoring and observability
3. **Optimize**: Tune performance based on metrics
4. **Scale**: Add partitions, consumers, compute as needed
5. **Evolve**: Move to advanced patterns as requirements grow

---

*Document created: 2024*
*Last updated: 2024*

